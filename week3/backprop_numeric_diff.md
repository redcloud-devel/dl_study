# Day 4: 역전파 직관 --- 수치 미분으로 확인하기

이 문서는 **역전파(backpropagation)의 의미를 수치 미분(numerical
differentiation)** 으로 확인하는 가장 기본적인 예제를 정리한 것이다.\
"기울기(gradient)가 왜 필요하고, 역전파가 무엇을 계산하는가"에 초점을
둔다.

------------------------------------------------------------------------

## 1. 모델 정의

아주 단순한 1층 선형 모델을 사용한다.

\[ y = w x + b \]

``` python
x = 3.0
y_true = 7.0

w = 1.0
b = 0.0
```

-   입력: (x = 3)
-   정답: (y\_{true} = 7)
-   현재 파라미터: (w=1, b=0)

예측값:

\[ y\_{pred} = 1 `\cdot 3`{=tex} + 0 = 3 \]

------------------------------------------------------------------------

## 2. 손실 함수 (Mean Squared Error)

``` python
def loss(w, b):
    y_pred = w * x + b
    return (y_pred - y_true) ** 2
```

수식으로는:

\[ L(w,b) = (wx + b - y\_{true})\^2 \]

현재 손실값:

\[ L(1,0) = (3 - 7)\^2 = 16 \]

------------------------------------------------------------------------

## 3. 왜 기울기가 필요한가?

학습은 보통 **경사하강법(Gradient Descent)** 으로 이루어진다.

\[ w `\leftarrow `{=tex}w -
`\eta `{=tex}`\frac{\partial L}{\partial w}`{=tex} \]

\[ b `\leftarrow `{=tex}b -
`\eta `{=tex}`\frac{\partial L}{\partial b}`{=tex} \]

------------------------------------------------------------------------

## 4. 수치 미분 (Central Difference)

``` python
h = 1e-5

dw = (loss(w + h, b) - loss(w - h, b)) / (2*h)
db = (loss(w, b + h) - loss(w, b - h)) / (2*h)
```

\[ `\frac{\partial L}{\partial w}`{=tex} `\approx`{=tex}
`\frac{L(w+h,b) - L(w-h,b)}{2h}`{=tex} \]

\[ `\frac{\partial L}{\partial b}`{=tex} `\approx`{=tex}
`\frac{L(w,b+h) - L(w,b-h)}{2h}`{=tex} \]

------------------------------------------------------------------------

## 5. 결과 해석

\[ wx + b - y = 3 - 7 = -4 \]

\[ `\frac{\partial L}{\partial w}`{=tex} = 2(-4) `\cdot 3`{=tex} = -24
\]

\[ `\frac{\partial L}{\partial b}`{=tex} = 2(-4) = -8 \]

수치 미분 결과와 정확히 일치해야 한다.

------------------------------------------------------------------------

## 6. 핵심 요약

-   역전파는 **손실의 정확한 기울기 계산**
-   수치 미분은 **검증용 도구**
-   학습에는 역전파만 사용
